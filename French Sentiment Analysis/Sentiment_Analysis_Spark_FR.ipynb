{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pyspark nltk"
      ],
      "metadata": {
        "id": "TH5H2-n77KcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment-fr"
      ],
      "metadata": {
        "id": "uRAoBiyhjcrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (regexp_replace, col, lower, current_timestamp, when, count, size, window, round, udf)\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "from pyspark.sql.types import DoubleType\n",
        "import pandas as pd\n",
        "from vaderSentiment_fr.vaderSentiment import SentimentIntensityAnalyzer # French version\n",
        "import nltk\n",
        "import os\n",
        "import glob\n",
        "from time import sleep\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from IPython.display import display, clear_output\n",
        "import re"
      ],
      "metadata": {
        "id": "lDOrRHCf7FYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"SentimentAnalysis\").getOrCreate()\n",
        "\n",
        "# Download VADER lexicon\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TPKd4wt7PBO",
        "outputId": "14b0eba2-b659-4ef1-f856-384aa5b8b167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove emojis\n",
        "def remove_emojis(text):\n",
        "    \"\"\"\n",
        "    Removes emojis from the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text from which emojis need to be removed.\n",
        "\n",
        "    Returns:\n",
        "        str: Text without emojis.\n",
        "    \"\"\"\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(df):\n",
        "    \"\"\"\n",
        "    Performs text cleaning operations such as converting to lowercase,\n",
        "    removing URLs, mentions, hashtags, special characters, and extra spaces.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input Spark DataFrame with text data.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Spark DataFrame with a cleaned text column.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        df.withColumn(\"cleaned_text\", lower(col(\"text\")))\n",
        "          .withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), r\"http\\S+\", \"\"))  # Remove URLs\n",
        "          .withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), r\"@\\w+\", \"\"))     # Remove mentions\n",
        "          .withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), r\"#\\w+\", \"\"))     # Remove hashtags\n",
        "          .withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), r\"[^a-zA-Z\\s]\", \"\"))  # Remove special characters\n",
        "          .withColumn(\"cleaned_text\", regexp_replace(col(\"cleaned_text\"), r\"\\s+\", \" \"))     # Remove extra spaces\n",
        "    )\n",
        "\n",
        "# Function to preprocess text: Tokenization and stop word removal\n",
        "def preprocess_text(df):\n",
        "    \"\"\"\n",
        "    Tokenizes the cleaned text and removes stop words. Filters out rows\n",
        "    with empty token lists.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input Spark DataFrame with cleaned text.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Spark DataFrame with tokens and filtered tokens.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"tokens\")\n",
        "    stop_words_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "    # Apply transformations\n",
        "    df_tokenized = tokenizer.transform(df)\n",
        "    df_filtered = stop_words_remover.transform(df_tokenized)\n",
        "\n",
        "    # Remove rows with empty tokens\n",
        "    return df_filtered.filter(size(col(\"filtered_tokens\")) > 0)\n",
        "\n",
        "# Define the VADER sentiment analysis UDF\n",
        "def analyze_sentiment_vader(text):\n",
        "    \"\"\"\n",
        "    Performs sentiment analysis using the VADER lexicon. Calculates a compound\n",
        "    score and classifies the sentiment as positive (1.0) or negative (0.0).\n",
        "\n",
        "    Args:\n",
        "        text (str or list): Input text or tokenized list of words.\n",
        "\n",
        "    Returns:\n",
        "        float: Sentiment classification, where 1.0 is positive and 0.0 is negative.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return 0  # if text is empty\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = sid.polarity_scores(' '.join(text) if isinstance(text, list) else text)\n",
        "    compound_score = sentiment_scores['compound']\n",
        "    return 1.0 if compound_score < -0.25 else 0.0 # Threshold is set to -0.25\n",
        "\n",
        "# Register UDF for Spark\n",
        "vader_sentiment_udf = udf(analyze_sentiment_vader, DoubleType())\n",
        "\n",
        "# Load and preprocess data\n",
        "def process_batch_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads the data from an Excel file, renames columns, and performs cleaning\n",
        "    and preprocessing.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the input Excel file.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Processed Spark DataFrame ready for sentiment analysis.\n",
        "    \"\"\"\n",
        "    # Read Excel file using pandas\n",
        "    pandas_df = pd.read_excel(file_path)\n",
        "\n",
        "    # Remove emojis from the text column\n",
        "    pandas_df['text'] = pandas_df['text'].apply(remove_emojis)\n",
        "\n",
        "    # Convert pandas DataFrame to Spark DataFrame\n",
        "    df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    df = df.dropDuplicates()\n",
        "\n",
        "    df = clean_text(df)\n",
        "    df = preprocess_text(df)\n",
        "    return df\n",
        "\n",
        "# Sentiment analysis and evaluation\n",
        "def apply_vader_sentiment(df):\n",
        "    \"\"\"\n",
        "    Applies the VADER sentiment analysis UDF to classify text sentiment.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Spark DataFrame with filtered tokens.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Spark DataFrame with an additional column for predicted sentiment.\n",
        "    \"\"\"\n",
        "    return df.withColumn(\"vader_predicted_label\", vader_sentiment_udf(col(\"filtered_tokens\")))"
      ],
      "metadata": {
        "id": "LxjZR8VH-P7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main analysis process\n",
        "file_path = \"/content/tweets_stream_final1.xlsx\"\n",
        "df_processed = process_batch_data(file_path)\n",
        "df_with_vader = apply_vader_sentiment(df_processed)\n",
        "\n",
        "# Only keep the needed column\n",
        "df_with_vader = df_with_vader.select(\"filtered_tokens\", \"vader_predicted_label\")\n",
        "\n",
        "# Since this is a test dataset without labels, we skip the accuracy calculation\n",
        "df_with_vader.show(20, truncate=False)"
      ],
      "metadata": {
        "id": "9gJtqfoK-VBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-time plotting function\n",
        "def plot_sentiment_analysis(aggregated_data):\n",
        "    \"\"\"\n",
        "    Generates and updates a real-time Plotly chart to display sentiment analysis results.\n",
        "\n",
        "    Args:\n",
        "        aggregated_data (list): List of dictionaries containing sentiment percentages\n",
        "                                and timestamps for visualization.\n",
        "    \"\"\"\n",
        "    plot_times, positive_percentages, negative_percentages = [], [], []\n",
        "    fig = make_subplots(rows=1, cols=1)\n",
        "    fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Positive Sentiment (%)', line=dict(color='green')))\n",
        "    fig.add_trace(go.Scatter(x=[], y=[], mode='lines+markers', name='Negative Sentiment (%)', line=dict(color='red')))\n",
        "    fig.update_layout(title='Sentiment Analysis Over Time', xaxis_title='Time', yaxis_title='Sentiment Percentage', template='plotly_white')\n",
        "\n",
        "    for entry in aggregated_data:\n",
        "        plot_times.append(entry['time'])\n",
        "        positive_percentages.append(entry['positive_percentage'])\n",
        "        negative_percentages.append(entry['negative_percentage'])\n",
        "\n",
        "        fig.data[0].x = plot_times\n",
        "        fig.data[0].y = positive_percentages\n",
        "        fig.data[1].x = plot_times\n",
        "        fig.data[1].y = negative_percentages\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        display(fig)"
      ],
      "metadata": {
        "id": "Edg5zpApBQ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated streaming processing (5 minutes interval)\n",
        "def simulated_streaming_processing(data, chunk_size, output_path, window_size=\"1 minutes\", toPlot=True, toAggregate=False):\n",
        "    \"\"\"\n",
        "    Processes data in a simulated streaming fashion. Aggregates sentiment analysis\n",
        "    results in time windows and optionally generates real-time plots.\n",
        "\n",
        "    Args:\n",
        "        data (DataFrame): Pandas DataFrame containing input data.\n",
        "        chunk_size (int): Number of rows to process per chunk.\n",
        "        output_path (str): Path to store aggregated results as a CSV file.\n",
        "        window_size (str): Time window size for aggregation.\n",
        "        toPlot (bool): If True, plots results in real-time.\n",
        "        toAggregate (bool): If True, displays aggregated results.\n",
        "    \"\"\"\n",
        "    start_time = pd.to_datetime(\"2024-11-30 20:20:00\")\n",
        "    end_time = start_time + pd.Timedelta(minutes=1)\n",
        "    aggregated_results = []\n",
        "\n",
        "    for i in range(0, len(data), chunk_size):\n",
        "        # Extract chunk\n",
        "        chunk = data.iloc[i:i + chunk_size]\n",
        "        chunk['timestamp'] = start_time\n",
        "        start_time = end_time\n",
        "        end_time = start_time + pd.Timedelta(minutes=1)\n",
        "\n",
        "        # Process data chunk\n",
        "        chunk_spark = spark.createDataFrame(chunk)\n",
        "        chunk_spark = clean_text(chunk_spark)\n",
        "        chunk_spark = preprocess_text(chunk_spark)\n",
        "\n",
        "        # Apply sentiment analysis\n",
        "        df_with_vader = apply_vader_sentiment(chunk_spark)\n",
        "        df_with_vader = df_with_vader.withColumn(\"processing_time\", current_timestamp())\n",
        "\n",
        "        # Aggregate in 5-minute windows\n",
        "        df_aggregated = (\n",
        "            df_with_vader\n",
        "            .groupBy(window(col(\"processing_time\"), window_size))\n",
        "            .agg(\n",
        "                count(when(col(\"vader_predicted_label\") == 1.0, True)).alias(\"positive_count\"),\n",
        "                count(when(col(\"vader_predicted_label\") == 0.0, True)).alias(\"negative_count\"),\n",
        "                count(\"*\").alias(\"total_count\")\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Calculate percentages\n",
        "        df_aggregated_with_percentages = df_aggregated.withColumn(\n",
        "            \"positive_percentage\", round((col(\"positive_count\") / col(\"total_count\")) * 100, 1)\n",
        "        ).withColumn(\n",
        "            \"negative_percentage\", round((col(\"negative_count\") / col(\"total_count\")) * 100, 1)\n",
        "        )\n",
        "\n",
        "        df_aggregated_with_times = df_aggregated_with_percentages.select(\n",
        "            \"window.start\", \"window.end\", \"positive_count\", \"negative_count\",\n",
        "            \"total_count\", \"positive_percentage\", \"negative_percentage\"\n",
        "        )\n",
        "\n",
        "        # Convert to Pandas DataFrame for storage or visualization\n",
        "        pandas_df = df_aggregated_with_times.toPandas()\n",
        "\n",
        "        if os.path.exists(output_path):\n",
        "            pandas_df.to_csv(output_path, mode='a', header=False, index=False)\n",
        "        else:\n",
        "            pandas_df.to_csv(output_path, mode='w', header=True, index=False)\n",
        "\n",
        "        # Append results for plotting\n",
        "        for _, row in pandas_df.iterrows():\n",
        "            aggregated_results.append({\n",
        "                'time': row['start'],\n",
        "                'positive_percentage': row['positive_percentage'],\n",
        "                'negative_percentage': row['negative_percentage']\n",
        "            })\n",
        "\n",
        "        # Whether to plot or aggregate (one at a time, since we're using cloud-based notebook)\n",
        "        if toAggregate:\n",
        "          df_aggregated_with_percentages.show(truncate=False)\n",
        "        elif toPlot:\n",
        "          plot_sentiment_analysis(aggregated_results)\n",
        "\n",
        "        # Delay of 5 minutes to simulate real-time\n",
        "        sleep(60)\n",
        "\n",
        "def clean_output_folder(output_folder):\n",
        "    \"\"\"\n",
        "    Cleans the output folder by removing all files.\n",
        "\n",
        "    Args:\n",
        "        output_folder (str): Path to the folder to be cleaned.\n",
        "    \"\"\"\n",
        "    files = glob.glob(f\"{output_folder}/*\")\n",
        "    for file in files:\n",
        "        os.remove(file)\n",
        "        print(f\"Deleted {file}\")"
      ],
      "metadata": {
        "id": "AA9D1cfM-rP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_output_folder(\"output\")\n",
        "pandas_df = pd.read_excel(file_path) # Read .xlsx file\n",
        "data = pd.DataFrame(pandas_df) # Convert to pandas DataFrame\n",
        "output_path = \"output/sentiment_results.csv\"\n",
        "# Aggregate the streaming data results\n",
        "simulated_streaming_processing(data, chunk_size=100, output_path=output_path, toPlot=False, toAggregate=True)"
      ],
      "metadata": {
        "id": "oG2fAB8--vts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_output_folder(\"output\")\n",
        "pandas_df = pd.read_excel(file_path)\n",
        "data = pd.DataFrame(pandas_df)\n",
        "output_path = \"output/sentiment_results.csv\"\n",
        "# Plot the streaming data results\n",
        "simulated_streaming_processing(data, chunk_size=100, output_path=output_path, toPlot=True, toAggregate=False)"
      ],
      "metadata": {
        "id": "IDUM5gbWKE_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "iCis20Fi_neO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}